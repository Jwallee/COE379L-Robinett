{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40769fe9-c0a2-4c69-af32-ca74e9bde9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 19:42:58.712420: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-16 19:42:59.898096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-16 19:43:02.757504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7da34c-5ed6-444e-a891-9d43625948ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_to_fr_translator = pipeline(\"translation_en_to_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2894b73f-4ee3-4495-9ffa-9d27df0d3ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Bonjour, mon nom est Joe.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_to_fr_translator(\"Hello, my name is Joe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbe4c4-d691-44f6-8efb-9f02fa8b10a1",
   "metadata": {},
   "source": [
    "## As we can see, it has a list of dictionaries. This allows BATCHING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854dbaa4-0d84-464b-aa82-92089087de02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'L’apprentissage automatique est une branche de l’intelligence artificielle'},\n",
       " {'translation_text': \"La Déclaration d'indépendance des États-Unis a été rédigée en 1776.\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_to_fr_translator([\"Machine learning is a branch of Artificial Intelligence\",\n",
    "                 \"The United States Declaration of Independence was written in 1776.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c853667d-53cd-461a-b6f9-b33e4c98356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd32ce5afc9c472ea6430021bd183780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8ce239b9a64b86b949b17528df4341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93bf596d517402fa383c584818032dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa362af5205347fb986552674b1acefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e71c36ff47b4c5c92c5ce72e05e79c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but your input_length is only 135. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=67)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' NLP is one of the oldest areas of AI and has a long history dating back at least to the 1950s . One of the first efforts to garner public attention was the Georgetown-IBM experiment in 1954, which attempted automatically translate Russian sentences to English . The goal was to simulate a psychotherapy session .'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"NLP is one of the oldest areas of AI and has a long history dating back at least to the 1950s.\n",
    "    One of the first efforts to garner public attention was the Georgetown-IBM experiment in 1954, which\n",
    "    attempted automatically translate Russian sentences to English.\n",
    "    Here is a screenshot from an early, famous NPL program called ELIZA, developed at MIT between 1964 and\n",
    "    1967. THe ELIZA program prompted users with questions in natural language text and enabled them to\n",
    "    submit answers, also in natural language. The goal was to simulate a psychotherapy session.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843524b2-9ce8-4d22-9ddc-1a459d3ac24e",
   "metadata": {},
   "source": [
    "# Look for models at Huggingface HUB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ed7472-3ed6-44b9-b167-31dbe1ae4895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f090811e101949c4b4f06802cc3a0b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf01a4f7bea6473a858d1bed854c7bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885a55727331425aa162e6212b5c089c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0366d03c0cd4eca9b1883853cab8a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9562a2c74f438facb0aecab0a89803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec28f20337184a34b07e3937f4f45e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc82e9265424814948548bf12bc3518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hola, mi nombre es Joe.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp_translator = pipeline(model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "en_sp_translator(\"Hello, my name is Joe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9a49b8-9e29-4b0f-a0a3-8455729c7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to ydshieh/vit-gpt2-coco-en and revision 65636df (https://huggingface.co/ydshieh/vit-gpt2-coco-en).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'a stuffed animal with a face on it '}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pipeline with the default model for the task\n",
    "image_to_text = pipeline('image-to-text')\n",
    "\n",
    "# use the model on an image; in this case we can simply pass it the path to a file\n",
    "image_to_text(\"data/monkie.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c3dd1-9ae7-468d-b6c0-e58fde89bbc9",
   "metadata": {},
   "source": [
    "# We can auto-tokenize using the tokenizer from what model we want to use!\n",
    "\n",
    "*Basically, get me the tokenizer from the pretrained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9270cd8b-f1a5-4024-93d6-be05a06ef0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2596e2ccc74340963c83bf4d8ce4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b7d49de93348ca9c43f8e4943db4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d574bac5b5f647a99fb9639024ce9c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec16fcd826e408eb9bd4d92fd230f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bb9ac5-0718-44bd-a7ec-b09c0ad1e0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2833, 2001, 2204, 1010, 2025, 2919, 2012, 2035, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "d = tokenizer(\"The food was good, not bad at all.\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4448ddf7-bc92-4e0a-8c93-2263864f0262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'food',\n",
       " 'was',\n",
       " 'good',\n",
       " ',',\n",
       " 'not',\n",
       " 'bad',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(d['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f1ae1c0-c133-446c-9461-b3c38c55f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2833, 2001, 2204, 1010, 2025, 2919, 2012, 2035, 1012, 102, 1996, 2833, 2001, 2919, 1010, 2025, 2204, 2012, 2035, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "d2 = tokenizer(\"The food was good, not bad at all.\", \"The food was bad, not good at all.\")\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f0912f-6b6a-4257-9bff-c16b956a5229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'food',\n",
       " 'was',\n",
       " 'good',\n",
       " ',',\n",
       " 'not',\n",
       " 'bad',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'the',\n",
       " 'food',\n",
       " 'was',\n",
       " 'bad',\n",
       " ',',\n",
       " 'not',\n",
       " 'good',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(d2['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274afb3-9a10-42dc-8c1d-f5de4703d3c2",
   "metadata": {},
   "source": [
    "### Batching inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "160a3433-6906-45c2-bf78-fc6859c2ddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1996, 2833, 2001, 2204, 1010, 2025, 2919, 2012, 2035, 1012, 102], [101, 1996, 2833, 2001, 2919, 1010, 2025, 2204, 2012, 2035, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"The food was good, not bad at all.\", \"The food was bad, not good at all.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189db77-0000-434e-b9fd-3e4700968b06",
   "metadata": {},
   "source": [
    "### **FOR THIS EXAMPLE, token_type_ids ARE BOTH LISTS OF 0, SINCE WITH BACTHING, THEY BOTH THINK THEY ARE THE FIRST SENTENCE!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47046b5-6f61-455b-af34-364acd69f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
