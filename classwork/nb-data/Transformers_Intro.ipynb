{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40769fe9-c0a2-4c69-af32-ca74e9bde9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 19:42:58.712420: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-16 19:42:59.898096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-16 19:43:02.757504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7da34c-5ed6-444e-a891-9d43625948ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_to_fr_translator = pipeline(\"translation_en_to_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2894b73f-4ee3-4495-9ffa-9d27df0d3ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Bonjour, mon nom est Joe.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_to_fr_translator(\"Hello, my name is Joe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbe4c4-d691-44f6-8efb-9f02fa8b10a1",
   "metadata": {},
   "source": [
    "## As we can see, it has a list of dictionaries. This allows BATCHING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854dbaa4-0d84-464b-aa82-92089087de02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'L’apprentissage automatique est une branche de l’intelligence artificielle'},\n",
       " {'translation_text': \"La Déclaration d'indépendance des États-Unis a été rédigée en 1776.\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_to_fr_translator([\"Machine learning is a branch of Artificial Intelligence\",\n",
    "                 \"The United States Declaration of Independence was written in 1776.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c853667d-53cd-461a-b6f9-b33e4c98356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd32ce5afc9c472ea6430021bd183780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8ce239b9a64b86b949b17528df4341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93bf596d517402fa383c584818032dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa362af5205347fb986552674b1acefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e71c36ff47b4c5c92c5ce72e05e79c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but your input_length is only 135. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=67)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' NLP is one of the oldest areas of AI and has a long history dating back at least to the 1950s . One of the first efforts to garner public attention was the Georgetown-IBM experiment in 1954, which attempted automatically translate Russian sentences to English . The goal was to simulate a psychotherapy session .'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"NLP is one of the oldest areas of AI and has a long history dating back at least to the 1950s.\n",
    "    One of the first efforts to garner public attention was the Georgetown-IBM experiment in 1954, which\n",
    "    attempted automatically translate Russian sentences to English.\n",
    "    Here is a screenshot from an early, famous NPL program called ELIZA, developed at MIT between 1964 and\n",
    "    1967. THe ELIZA program prompted users with questions in natural language text and enabled them to\n",
    "    submit answers, also in natural language. The goal was to simulate a psychotherapy session.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843524b2-9ce8-4d22-9ddc-1a459d3ac24e",
   "metadata": {},
   "source": [
    "# Look for models at Huggingface HUB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ed7472-3ed6-44b9-b167-31dbe1ae4895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f090811e101949c4b4f06802cc3a0b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf01a4f7bea6473a858d1bed854c7bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885a55727331425aa162e6212b5c089c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0366d03c0cd4eca9b1883853cab8a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9562a2c74f438facb0aecab0a89803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec28f20337184a34b07e3937f4f45e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc82e9265424814948548bf12bc3518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hola, mi nombre es Joe.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp_translator = pipeline(model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "en_sp_translator(\"Hello, my name is Joe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9a49b8-9e29-4b0f-a0a3-8455729c7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to ydshieh/vit-gpt2-coco-en and revision 65636df (https://huggingface.co/ydshieh/vit-gpt2-coco-en).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'a stuffed animal with a face on it '}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pipeline with the default model for the task\n",
    "image_to_text = pipeline('image-to-text')\n",
    "\n",
    "# use the model on an image; in this case we can simply pass it the path to a file\n",
    "image_to_text(\"data/monkie.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c3dd1-9ae7-468d-b6c0-e58fde89bbc9",
   "metadata": {},
   "source": [
    "# We can auto-tokenize using the tokenizer from what model we want to use!\n",
    "\n",
    "*Basically, get me the tokenizer from the pretrained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9270cd8b-f1a5-4024-93d6-be05a06ef0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2596e2ccc74340963c83bf4d8ce4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b7d49de93348ca9c43f8e4943db4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d574bac5b5f647a99fb9639024ce9c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec16fcd826e408eb9bd4d92fd230f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bb9ac5-0718-44bd-a7ec-b09c0ad1e0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2833, 2001, 2204, 1010, 2025, 2919, 2012, 2035, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "d = tokenizer(\"The food was good, not bad at all.\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4448ddf7-bc92-4e0a-8c93-2263864f0262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'food',\n",
       " 'was',\n",
       " 'good',\n",
       " ',',\n",
       " 'not',\n",
       " 'bad',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(d['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f1ae1c0-c133-446c-9461-b3c38c55f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2833, 2001, 2204, 1010, 2025, 2919, 2012, 2035, 1012, 102, 1996, 2833, 2001, 2919, 1010, 2025, 2204, 2012, 2035, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "d2 = tokenizer(\"The food was good, not bad at all.\", \"The food was bad, not good at all.\")\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f0912f-6b6a-4257-9bff-c16b956a5229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'food',\n",
       " 'was',\n",
       " 'good',\n",
       " ',',\n",
       " 'not',\n",
       " 'bad',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'the',\n",
       " 'food',\n",
       " 'was',\n",
       " 'bad',\n",
       " ',',\n",
       " 'not',\n",
       " 'good',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(d2['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274afb3-9a10-42dc-8c1d-f5de4703d3c2",
   "metadata": {},
   "source": [
    "### Batching inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "160a3433-6906-45c2-bf78-fc6859c2ddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1996, 2833, 2001, 2204, 1010, 2025, 2919, 2012, 2035, 1012, 102], [101, 1996, 2833, 2001, 2919, 1010, 2025, 2204, 2012, 2035, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"The food was good, not bad at all.\", \"The food was bad, not good at all.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189db77-0000-434e-b9fd-3e4700968b06",
   "metadata": {},
   "source": [
    "### **FOR THIS EXAMPLE, token_type_ids ARE BOTH LISTS OF 0, SINCE WITH BACTHING, THEY BOTH THINK THEY ARE THE FIRST SENTENCE!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514fa3f-219a-49b4-b3d3-ec38aaf423a7",
   "metadata": {},
   "source": [
    "Pytorch Tensor Conversion for the input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e684456-4211-4a06-9fb2-fb644fec9d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1996, 2833, 2001, 2204, 1010, 2025, 2919, 2012, 2035,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<class 'torch.Tensor'> torch.Size([1, 11])\n",
      "tensor([[ 101, 1996, 2833, 2001, 2204, 1010, 2025, 2919, 2012, 2035,  102]])\n"
     ]
    }
   ],
   "source": [
    "# 'pt' converts it to pytorch format\n",
    "d = tokenizer(\"The food was good, not bad at all\", return_tensors=\"pt\")\n",
    "print(d)\n",
    "tensors = d['input_ids']\n",
    "print(type(tensors), tensors.shape)\n",
    "print(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aba9b48-8a6b-47dd-86da-5d0c107d7b03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 6 at dim 1 (got 12)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe food was good\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe food was bad, not good at all.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2829\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2828\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2829\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2915\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2911\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2912\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2913\u001b[0m         )\n\u001b[1;32m   2914\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2932\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2936\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2937\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2953\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2954\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3106\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3098\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3099\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3103\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3104\u001b[0m )\n\u001b[0;32m-> 3106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3108\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3123\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:552\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:764\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    760\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    761\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    762\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    768\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    769\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "tokenizer([\"The food was good\", \"The food was bad, not good at all.\"], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0eefff-15e8-49d8-ba6f-07c414134212",
   "metadata": {},
   "source": [
    "### ^*This blows up because the set of tokens are different lengths. ANNs (this transformer) wants a rectangular shape*\n",
    "\n",
    "To fix this, we pad the shorter one with '0' to make it match the longer one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb51af5b-12b4-4ca6-b0e1-aab1d95d7b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 2833, 2001, 2204,  102,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 1996, 2833, 2001, 2919, 1010, 2025, 2204, 2012, 2035, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we properly pad our input data\n",
    "tokenizer([\"The food was good\", \"The food was bad, not good at all.\"], return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d3e29-8579-4398-b560-27f0020ce36a",
   "metadata": {},
   "source": [
    "### OMG!!! As we can see, it padded properly! However, we don't want to send in a bunch of '0' garbage data to the model, so that is where ***attention_mask*** comes into play! It tells the model what it should not pay attention to, i.e. the padded zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99520bf6-a780-4a8f-8925-3009dbac3242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'food',\n",
       " 'was',\n",
       " 'good',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3 = tokenizer([\"The food was good\", \"The food was bad, not good at all.\"], padding=True)\n",
    "tokenizer.convert_ids_to_tokens(d3['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21211590-aeee-4e4c-bfa2-c62bcb56f4ad",
   "metadata": {},
   "source": [
    "## **Models from Checkpoints and Language Embeddings**\n",
    "\n",
    "Use the 'automodel' class to use a pretrained checkpoint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdd2cf88-ab5e-472a-b150-4be2151c414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27319a864a7d485ba53ca7162d3fe2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-4.7362e-02, -7.8231e-02, -1.7756e-01,  ..., -6.8721e-02,\n",
       "           5.2925e-01,  3.5048e-01],\n",
       "         [-3.8071e-01, -8.8502e-01, -7.5860e-01,  ..., -1.4404e-01,\n",
       "           1.0165e+00,  3.9322e-01],\n",
       "         [ 4.9478e-02, -4.5846e-01,  4.7081e-01,  ..., -2.9033e-04,\n",
       "           4.1519e-02,  6.8883e-02],\n",
       "         ...,\n",
       "         [-8.4983e-01, -3.9531e-01,  8.3996e-01,  ..., -2.8305e-03,\n",
       "          -3.0003e-01,  1.7564e-01],\n",
       "         [-1.6971e+00, -3.9712e-01, -2.4921e-01,  ...,  2.7338e-01,\n",
       "           6.5613e-01, -1.1496e-01],\n",
       "         [ 5.2601e-01, -6.0673e-02, -3.1021e-01,  ...,  4.4031e-01,\n",
       "          -4.1592e-01, -3.9783e-01]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8641, -0.4517, -0.9026,  0.7122,  0.6778, -0.0026,  0.8965,  0.2366,\n",
       "         -0.8062, -1.0000, -0.5332,  0.9594,  0.9606,  0.3088,  0.9340, -0.5803,\n",
       "         -0.1417, -0.5231,  0.2528, -0.3736,  0.7360,  1.0000,  0.0706,  0.3547,\n",
       "          0.3408,  0.9873, -0.7121,  0.9050,  0.9369,  0.6879, -0.5627,  0.1685,\n",
       "         -0.9828, -0.0348, -0.8593, -0.9840,  0.2155, -0.6968,  0.1255,  0.0818,\n",
       "         -0.8844,  0.2120,  1.0000, -0.3040,  0.3601, -0.2025, -1.0000,  0.1619,\n",
       "         -0.8268,  0.8909,  0.8759,  0.8700,  0.0391,  0.4310,  0.4200,  0.0263,\n",
       "         -0.2089,  0.0657, -0.1615, -0.5802, -0.6083,  0.2740, -0.8420, -0.8999,\n",
       "          0.9213,  0.6491, -0.0590, -0.3246, -0.0380, -0.1029,  0.9078,  0.1377,\n",
       "         -0.1194, -0.8263,  0.6667,  0.2745, -0.6176,  1.0000, -0.2398, -0.9682,\n",
       "          0.9025,  0.7765,  0.5115, -0.2984,  0.2928, -1.0000,  0.6154, -0.0800,\n",
       "         -0.9836,  0.2053,  0.6480, -0.2546,  0.6887,  0.5797, -0.6264, -0.4402,\n",
       "         -0.3072, -0.7632, -0.1536, -0.2399, -0.0113, -0.0542, -0.3126, -0.3234,\n",
       "          0.0965, -0.4982, -0.4235,  0.3466,  0.1690,  0.6229,  0.4383, -0.2923,\n",
       "          0.3069, -0.9429,  0.5855, -0.3061, -0.9791, -0.6341, -0.9872,  0.6892,\n",
       "         -0.3614, -0.2624,  0.9432, -0.2529,  0.3729,  0.0530, -0.9043, -1.0000,\n",
       "         -0.6714, -0.5426, -0.1582, -0.2750, -0.9604, -0.9493,  0.5349,  0.9365,\n",
       "          0.2366,  0.9999, -0.2621,  0.9376, -0.4024, -0.5286,  0.6548, -0.4620,\n",
       "          0.6309,  0.3082, -0.4819,  0.2116, -0.3483,  0.5257, -0.8013, -0.0466,\n",
       "         -0.7672, -0.9392, -0.2600,  0.9160, -0.5452, -0.9087, -0.1007, -0.0878,\n",
       "         -0.5101,  0.7677,  0.7198,  0.4085, -0.1707,  0.4098,  0.2926,  0.5427,\n",
       "         -0.7329, -0.1530,  0.3726, -0.2081, -0.7908, -0.9736, -0.3599,  0.4779,\n",
       "          0.9770,  0.6937,  0.1859,  0.8047, -0.1990,  0.7680, -0.9221,  0.9664,\n",
       "         -0.1537,  0.2520, -0.4747,  0.4691, -0.8163,  0.0085,  0.8151, -0.8539,\n",
       "         -0.7237, -0.0430, -0.3373, -0.3020, -0.7487,  0.4388, -0.2757, -0.3453,\n",
       "         -0.0789,  0.9172,  0.9603,  0.7756,  0.2296,  0.5404, -0.8834, -0.5170,\n",
       "         -0.0135,  0.0429,  0.0202,  0.9893, -0.6804, -0.0613, -0.9054, -0.9791,\n",
       "         -0.1220, -0.8572, -0.0938, -0.6567,  0.5479, -0.4825,  0.5656,  0.2904,\n",
       "         -0.9763, -0.6917,  0.2881, -0.4105,  0.3927, -0.3300,  0.8208,  0.9248,\n",
       "         -0.5655,  0.5368,  0.8939, -0.9405, -0.7580,  0.7730, -0.2881,  0.7653,\n",
       "         -0.6880,  0.9830,  0.9190,  0.8205, -0.8899, -0.7903, -0.8103, -0.7342,\n",
       "         -0.0606,  0.0230,  0.8820,  0.6211,  0.2973,  0.4776, -0.5734,  0.9930,\n",
       "         -0.9081, -0.9096, -0.8353, -0.3030, -0.9783,  0.8964,  0.3502,  0.5744,\n",
       "         -0.4241, -0.6826, -0.9558,  0.7534,  0.0721,  0.9795, -0.1862, -0.8949,\n",
       "         -0.5319, -0.8755, -0.2469, -0.1580, -0.4104, -0.2253, -0.9489,  0.5006,\n",
       "          0.5109,  0.4989, -0.7831,  0.9973,  1.0000,  0.9553,  0.8536,  0.8954,\n",
       "         -0.9999, -0.7183,  1.0000, -0.9860, -1.0000, -0.8997, -0.6930,  0.3678,\n",
       "         -1.0000, -0.1485,  0.0254, -0.8913,  0.7542,  0.9579,  0.9873, -1.0000,\n",
       "          0.8236,  0.9203, -0.6193,  0.9617, -0.2015,  0.9454,  0.6318,  0.5201,\n",
       "         -0.2069,  0.4134, -0.9361, -0.8163, -0.5909, -0.7229,  0.9973,  0.1590,\n",
       "         -0.7868, -0.8350,  0.4749, -0.1377, -0.1502, -0.9595, -0.1133,  0.6350,\n",
       "          0.8336,  0.1282,  0.3318, -0.5690,  0.1096, -0.0362,  0.2907,  0.6463,\n",
       "         -0.9234, -0.4281, -0.1257, -0.0589, -0.6353, -0.9473,  0.9538, -0.4717,\n",
       "          0.8316,  1.0000,  0.5556, -0.8330,  0.5334,  0.2102, -0.5615,  1.0000,\n",
       "          0.6433, -0.9567, -0.5585,  0.4431, -0.5895, -0.4812,  0.9992, -0.1197,\n",
       "         -0.6696, -0.6174,  0.9681, -0.9758,  0.9920, -0.8532, -0.9466,  0.9697,\n",
       "          0.9277, -0.7059, -0.7084,  0.1599, -0.5509,  0.4004, -0.9396,  0.7473,\n",
       "          0.3535,  0.0306,  0.8593, -0.7591, -0.5598,  0.2878, -0.6205, -0.4099,\n",
       "          0.9324,  0.4615, -0.1794, -0.1299, -0.2290, -0.7634, -0.9635,  0.4135,\n",
       "          1.0000, -0.3336,  0.7944, -0.4879,  0.0332, -0.1331,  0.4800,  0.4940,\n",
       "         -0.3033, -0.8631,  0.7973, -0.9553, -0.9797,  0.8097,  0.2600, -0.2290,\n",
       "          1.0000,  0.5568,  0.1340,  0.3809,  0.9870, -0.1124,  0.3969,  0.8501,\n",
       "          0.9582, -0.1412,  0.5958,  0.7956, -0.8895, -0.1137, -0.6306, -0.0380,\n",
       "         -0.9193,  0.1942, -0.9511,  0.9369,  0.8876,  0.2441,  0.1492,  0.6299,\n",
       "          1.0000, -0.8892,  0.5822,  0.0758,  0.7051, -0.9998, -0.8058, -0.4658,\n",
       "          0.0165, -0.8263, -0.2743,  0.2270, -0.9606,  0.7680,  0.5753, -0.9749,\n",
       "         -0.9863, -0.3238,  0.8669,  0.0645, -0.9829, -0.6702, -0.4641,  0.7771,\n",
       "         -0.2515, -0.9255, -0.1521, -0.2366,  0.4345, -0.1577,  0.5736,  0.7888,\n",
       "          0.8278, -0.7034, -0.5588, -0.2314, -0.7914,  0.7289, -0.6915, -0.9364,\n",
       "         -0.1285,  1.0000, -0.4820,  0.9097,  0.6800,  0.5125, -0.0849,  0.0488,\n",
       "          0.9115,  0.2415, -0.7173, -0.7974, -0.2328, -0.2825,  0.5505,  0.6979,\n",
       "          0.5809,  0.7762,  0.8407,  0.0621, -0.0575, -0.0323,  0.9989, -0.1119,\n",
       "         -0.2137, -0.3782, -0.0389, -0.3167,  0.2280,  1.0000,  0.2960,  0.5811,\n",
       "         -0.9799, -0.8759, -0.8464,  1.0000,  0.7949, -0.8248,  0.6413,  0.6865,\n",
       "         -0.0744,  0.6188, -0.0825, -0.2567,  0.1017,  0.1224,  0.9242, -0.6019,\n",
       "         -0.9578, -0.4899,  0.4328, -0.9510,  0.9999, -0.6364, -0.2438, -0.4875,\n",
       "         -0.2743, -0.2546, -0.0256, -0.9691, -0.2235,  0.1111,  0.9359,  0.1216,\n",
       "         -0.5581, -0.8611,  0.7352,  0.8046, -0.9150, -0.9281,  0.9417, -0.9674,\n",
       "          0.6447,  1.0000,  0.2856,  0.2818,  0.2591, -0.3263,  0.4246, -0.5505,\n",
       "          0.6541, -0.9436, -0.2612, -0.0909,  0.2400,  0.0327, -0.5316,  0.6634,\n",
       "          0.0652, -0.4967, -0.6288,  0.0086,  0.4129,  0.8595, -0.1122, -0.1744,\n",
       "         -0.0454, -0.0117, -0.8960, -0.3306, -0.3444, -1.0000,  0.6210, -1.0000,\n",
       "          0.4062,  0.0965, -0.1510,  0.8499,  0.7316,  0.7039, -0.6391, -0.7816,\n",
       "          0.3933,  0.7443, -0.1614, -0.3221, -0.5177,  0.3074, -0.0509,  0.2974,\n",
       "         -0.2980,  0.7044, -0.1879,  1.0000,  0.1195, -0.5791, -0.9775,  0.1417,\n",
       "         -0.2145,  1.0000, -0.7324, -0.9248,  0.2685, -0.5761, -0.8072,  0.3408,\n",
       "         -0.1677, -0.8075, -0.9591,  0.9325,  0.7817, -0.5888,  0.4027, -0.2210,\n",
       "         -0.5368, -0.1643,  0.8533,  0.9807,  0.6910,  0.8320, -0.1708, -0.3615,\n",
       "          0.9574,  0.0643,  0.0381,  0.0628,  1.0000,  0.2489, -0.9121, -0.2679,\n",
       "         -0.9783, -0.0569, -0.8954,  0.4102,  0.2287,  0.8581, -0.2919,  0.9219,\n",
       "         -0.8980, -0.0891, -0.4811, -0.3830,  0.3646, -0.9163, -0.9702, -0.9783,\n",
       "          0.5203, -0.3636,  0.0599,  0.1821, -0.0977,  0.3913,  0.4336, -1.0000,\n",
       "          0.9127,  0.3376,  0.8581,  0.9450,  0.7663,  0.5186,  0.2554, -0.9728,\n",
       "         -0.9760, -0.3359, -0.3099,  0.6871,  0.5209,  0.8620,  0.3920, -0.3521,\n",
       "         -0.6909, -0.6771, -0.8969, -0.9858,  0.4097, -0.6371, -0.8850,  0.9437,\n",
       "         -0.0642, -0.1002, -0.1885, -0.8432,  0.8879,  0.8193,  0.0173,  0.0364,\n",
       "          0.3796,  0.8713,  0.9147,  0.9627, -0.8361,  0.6274, -0.6462,  0.4324,\n",
       "          0.8966, -0.9028,  0.1539,  0.2624, -0.2496,  0.1555, -0.1728, -0.9481,\n",
       "          0.6726, -0.2129,  0.5300, -0.4404,  0.0797, -0.3016, -0.0590, -0.7200,\n",
       "         -0.7304,  0.5805,  0.1661,  0.8867,  0.8475,  0.0850, -0.6234, -0.1387,\n",
       "         -0.7411, -0.9028,  0.8908,  0.0016, -0.1798,  0.4954, -0.0929,  0.9621,\n",
       "          0.2419, -0.3297, -0.2302, -0.5384,  0.8318, -0.6743, -0.4219, -0.5897,\n",
       "          0.7843,  0.3027,  1.0000, -0.7645, -0.9202, -0.4001, -0.3938,  0.3845,\n",
       "         -0.5874, -1.0000,  0.2805, -0.6792,  0.7333, -0.7784,  0.8852, -0.6614,\n",
       "         -0.9770, -0.3311,  0.6198,  0.7800, -0.4632, -0.6412,  0.5881, -0.1193,\n",
       "          0.9817,  0.8297,  0.1060,  0.0043,  0.6701, -0.7319, -0.6282,  0.9156]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "model(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcd6882c-c11c-4035-a63b-22d2c12f683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
      "         [-0.0446,  0.0061, -0.0022,  ..., -0.0363, -0.0004, -0.0306],\n",
      "         [-0.0179, -0.0035, -0.0022,  ..., -0.0005,  0.0112, -0.0379],\n",
      "         ...,\n",
      "         [-0.0397, -0.0034, -0.0047,  ..., -0.0252, -0.0508,  0.0202],\n",
      "         [-0.0546,  0.0065, -0.0213,  ...,  0.0427,  0.0057, -0.0381],\n",
      "         [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings.word_embeddings(tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954b1f1-181d-457a-ac08-a52f2844185b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
